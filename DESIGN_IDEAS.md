# Spanish Vibes â€” Design Ideas & Architecture

This file captures the design thinking behind Spanish Vibes. Sections marked âœ… are implemented. Sections marked ðŸ’¡ are ideas not yet built.

---

## âœ… Persona System ("Souls")

### What's built
Each conversation partner is a distinct character with personality, interests, memories, and a consistent voice. Four personas live in `data/personas/` as YAML soul files:

- **Marta** â€” friendly teacher-type, patient, asks follow-up questions
- **Diego** â€” football-obsessed uni student, uses slang, high energy
- **Abuela Rosa** â€” warm grandmother, talks about cooking and family, traditional expressions
- **Luis** â€” tech startup guy, talks fast, mixes in English loanwords

Each YAML defines identity, personality traits, conversation style, interest weights, vocab level, backstory, and a system prompt template with injection slots for `{persona_memories}`, `{user_profile}`, and `{concept_focus}`.

### How it works
- `personas.py` loads YAML files, builds dynamic system prompts with memory injection
- `select_persona()` uses engagement-weighted random selection â€” personas with higher enjoyment scores get picked more often, with a novelty bonus for less-seen personas
- `get_persona_prompt()` injects retrieved memories and user facts into the system prompt before each conversation
- Conversations reference a persona_id, so the system knows who said what

### ðŸ’¡ Future ideas
- **More personas** â€” A sarcastic teenager, a professor type, a traveler. Each unlocks naturally as the system learns what the user responds to
- **Persona unlocking** â€” Could gate some personas behind level thresholds (Diego's slang is hard for A1 learners)
- **Voice differentiation** â€” If/when TTS is added, each persona gets a distinct voice style
- **"Your Spanish friends" dashboard** â€” Show persona affinity scores, conversation count per persona, fun stats

---

## âœ… Memory System

### What's built
Two complementary memory stores:

**Persona memories** (`persona_memories` table) â€” What each persona remembers from past conversations. Capped at ~20 per persona, pruned by importance + recency. Examples: "User mentioned they have a dog named Max", "User struggled with ser/estar for locations."

**User profile** (`user_profile` table) â€” Facts about the user, shared across all personas. Key-value pairs with confidence scores. Examples: "Has a dog", "Likes Italian food", "Lives in Melbourne."

Both are extracted via the post-conversation evaluation LLM call and injected into persona system prompts before each new conversation.

### ðŸ’¡ Future ideas
- **Memory conflicts** â€” What happens when the user tells Marta they're vegetarian but tells Diego they love steak? Could be handled with confidence decay or explicit contradiction detection
- **Learning preference tracking** â€” "User prefers gentle corrections", "User likes humor in conversations"
- **Memory-driven conversation starters** â€” Persona opens with a callback: "Â¿CÃ³mo estÃ¡ Max?" instead of generic greetings

---

## âœ… Making Conversations Fun

### What's built

**Conversation types** â€” Six distinct modes, not just free-form Q&A:

1. **General chat** (~50%) â€” Free-form, persona-driven. Best for building rapport and discovering interests
2. **Role play** (~20%) â€” Scenario-based (ordering food, asking directions, job interview). Curated scenarios per topic
3. **Concept-required** (~15%) â€” System picks concepts the user needs to practice, steers conversation to require them. Binary pass/fail on target concept production
4. **Tutor** (~15%) â€” Persona explicitly teaches a concept, with their personality flavoring the examples
5. **Story comprehension** â€” Persona tells a short story, user answers comprehension questions (UI scaffolding exists, generation logic TBD)
6. **Placement** â€” Special onboarding mode for calibrating new users

Type selection is weighted and adapts: if a concept is stuck, bump concept-required frequency. If engagement drops, more fun types (chat, role play).

**Enjoyment scoring** â€” TikTok-style behavioral inference, no explicit feedback needed:
```
enjoyment_score = weighted sum of:
  message_length_norm:  0.35
  completion_ratio:     0.25
  no_early_exit:        0.20
  response_time_score:  0.10
  engagement_quality:   0.10  (from LLM evaluation)
```

**Persona engagement tracking** (`persona_engagement` table) â€” Per personaÃ—topic: conversation count, average enjoyment, average message length, average turns, early exit rate. Feeds into persona selection weighting.

### ðŸ’¡ Future ideas
- **Running jokes / callbacks** â€” Built on memory system. Persona references something funny from a past conversation
- **Progression in relationship** â€” Early convos more formal, later ones feel like talking to a friend
- **Cliffhangers** â€” "Te cuento algo increÃ­ble..." then session ends. Next time picks up the thread
- **Collaborative storytelling** â€” "I start a story, you continue it" mode
- **Games within conversation** â€” 20 questions, would-you-rather, this-or-that
- **Mutual exchange** â€” Persona shares opinions and stories proactively, not just asking questions

---

## âœ… Post-Conversation Evaluation (The Hub)

### What's built

The architectural insight from the original design proved correct: almost everything flows through one moment â€” the post-conversation evaluation. After each conversation, a single GPT-4o call extracts:

```json
{
  "concepts_demonstrated": [{"concept_id": "ser_present", "correct": 2, "errors": [...]}],
  "vocabulary_used": ["nuevo", "trabajar", "cocina"],
  "user_facts": ["Has a dog named Max"],
  "persona_observations": ["User seemed excited about football"],
  "engagement_quality": 0.75,
  "estimated_cefr": "A2"
}
```

This feeds: BKT updates (with boosted weight for production evidence), word harvesting, memory storage, persona engagement tracking, interest signals, and enjoyment scoring. One call, six downstream systems.

---

## âœ… Adaptive Difficulty & Placement

### What's built

**Computed user level** â€” `get_user_level()` in `flow.py` computes a global level from BKT concept mastery across tiers:
- Tier 1 mastery < 50% â†’ level 1, CEFR A1
- Tier 1 â‰¥ 80%, Tier 2 < 50% â†’ level 2, CEFR A1-A2
- Tier 2 â‰¥ 80% â†’ level 3, CEFR A2-B1

This drives MCQ difficulty-aware selection and conversation scaffolding (CEFR level in system prompts).

**Placement conversation** â€” Special onboarding flow: 2-3 quick interest questions â†’ placement conversation where persona probes increasing complexity â†’ post-placement evaluation mass-unlocks concepts the user already knows.

### ðŸ’¡ Multi-Dimensional Profiling (Not Yet Built)

The current system uses a single global level. The original design called for tracking four independent dimensions:

1. **Vocabulary depth per interest** â€” You might know 100 sports words but only 10 food words. Vocabulary shaped by what you care about.
2. **Grammar accuracy** â€” BKT mastery + conversation correction patterns
3. **Conversational fluency** â€” Message length, response speed, sentence complexity
4. **Comprehension** â€” Can they understand input? Appropriate responses, clarification requests

Each dimension would have its own mastery curve. Content selection would target the weakest dimension while not boring the learner on strong ones.

**Heritage speaker mode** â€” For someone who speaks but never studied: high vocabulary + high fluency + low grammar awareness. System would skip all basic vocab, focus entirely on grammar through the lens of "you already say this â€” here's WHY it works."

This is the most ambitious unrealized idea. The infrastructure is mostly there (evaluation already extracts concepts, vocabulary, engagement quality), but the card selection engine doesn't yet differentiate by dimension.

---

## âœ… Interest System

### What's built

**Interest tracking** â€” Exponential moving average scoring with signal weighting:
- Correctness: 0.40, Dwell time: 0.30, Return frequency: 0.15, Progression: 0.10, Continuation: 0.05
- Struggle detection: long dwell + wrong answer = don't boost (prevents false positives from frustration)
- Time decay: 45-day half-life (interests fade without interaction)

**21 interest topics** seeded: Sports, Football, Technology, Music, Food & Cooking, Travel, Movies & TV, Science, Politics & News, Fashion, Gaming, Fitness, Business, Art, History, Nature & Animals, Relationships, Literature, Health, Cars, Photography.

**Signal source design** â€” Conversations produce interest signals (they reflect genuine engagement). MCQs consume interests (word/topic selection) but don't produce signals (they're assigned, not chosen).

**Concept-to-topic mapping** â€” `CONCEPT_TOPIC_MAP` in `interest.py` links 15 concepts to topic slugs (food_vocabâ†’food-cooking, animals_vocabâ†’nature-animals, etc.).

### ðŸ’¡ Future ideas
- **Deeper topic hierarchy** â€” Football under Sports, Italian food under Food & Cooking. Lets the system get more specific over time
- **Cross-persona interest triangulation** â€” If the user loves sports with Diego AND loves sports with Marta, that's stronger signal than just one persona
- **Interest-driven conversation topics** â€” Already partially wired, but could be more aggressive about surfacing topics the user loves

---

## âœ… Word System

### What's built

**531 seed words** across all 61 concepts in `data/seed_words.json`. Words have Spanish, English, emoji, example sentence, concept_id, and topic_slug for interest-driven prioritization.

**Word lifecycle** â€” Multiple entry points:
- **Seed words** â†’ unseen â†’ intro card â†’ introduced â†’ practice card â†’ known (2 correct = known)
- **Conversation production** â†’ practicing (skip intro! production > recognition) â†’ known
- **Word tap** (lookup during conversation) â†’ unseen â†’ prioritized for intro/practice â†’ known
- **English fallback gap** â†’ unseen â†’ existing pipeline â†’ known

**Interest-aware selection** â€” `get_intro_candidate_weighted()` prefers unseen words from high-interest topics. If the user loves sports, sports vocabulary surfaces before photography vocabulary.

**Translation pipeline** â€” 3-tier fallback: word_translations cache â†’ bundled es_en_dictionary.json (~5000 entries) â†’ GPT-4o-mini AI translation.

**Conversation harvesting** â€” After each conversation, all meaningful Spanish words the user produced are extracted, stop words filtered, and added to the words table. Existing words get times_correct bumped (production evidence).

**Word tap tracking** â€” Every tap recorded in `word_taps` table with full audit trail. Tapped words enter as 'unseen' (they looked it up because they don't know it).

### ðŸ’¡ Interest-Driven Vocabulary (The Virtuous Cycle)

The big design vision for vocabulary that's partially wired but not fully realized:

**The cycle:**
- High interest in basketball â†’ more basketball conversations with Diego
- Basketball conversations surface new basketball vocabulary naturally
- New words get introduced via word intro cards (interest-weighted)
- Richer vocabulary â†’ deeper basketball conversations â†’ more engagement â†’ system learns you love basketball even more
- Meanwhile photography vocabulary stays shallow because you never engage with it â€” and that's fine

**Vocabulary tiers per topic:**
- **Core** (everyone learns): universal words â€” greetings, numbers, basic verbs, pronouns, question words. topic_slug = null
- **Functional** (light exposure): enough to survive any topic â€” 10-20 words per domain
- **Deep** (interest-driven): rich vocabulary for topics you care about â€” 50-100+ words, idioms, slang

**What's still needed:**
- **Word-aware MCQ generation** â€” Tell the AI which words the user knows so distractors use known vocabulary
- **More aggressive interest weighting** â€” Current system just prefers high-interest words within a concept. Could go further: choose which CONCEPT to drill based on interest alignment
- **Deep vocabulary seeding** â€” Current 531 words are functional-level. For deep engagement with a topic, need specialized vocabulary beyond what's in concept teach_content

---

## ðŸ’¡ New Card Types

### Built
- **Teach card** â€” Concept introduction with formatted content
- **MCQ** â€” AI-generated multiple choice, difficulty 1-3
- **Word intro** â€” Show a new word with emoji, translation, example
- **Word practice** â€” Fill-in-the-blank with word
- **Word match** â€” Match Spanish-English pairs
- **Conversation** â€” Full chat with persona (6 types)

### Not yet built
- **Story comprehension** â€” Persona tells a short story (3-5 sentences) using target grammar, then 2-3 comprehension MCQs. UI scaffolding exists in templates, needs generation logic. Variations: retell mode, fill-in-the-story, continuing the story.
- **Fill-in-the-blank** â€” Full sentence with one word blanked, 4 choices. Context constrains the answer. Different from MCQ because it's inline and tests reading comprehension in context.
- **Sentence builder** â€” Scrambled words, arrange in correct order. Tests word order / grammar understanding differently.
- **Listening card** (future) â€” Play a sentence via TTS, pick the translation or type what you heard. Browser SpeechSynthesis API is free.
- **Image/emoji association** â€” Show an emoji or simple image, pick the Spanish word. Fast, visual, low-stakes.

---

## ðŸ’¡ AI Backend: Future Considerations

### Current state
- **GPT-4o** for conversation corrections and post-conversation evaluation
- **GPT-4o-mini** for MCQ generation, conversation chat, word translation fallback
- Works well for prototyping and small user base

### Future options when scaling
- **Fine-tuned small models** per persona â€” cheaper inference, more consistent personality. Need 1000+ conversation transcripts
- **Hybrid** â€” small model for chat, large model for evaluation/memory extraction
- **Self-hosted** (Llama/Mistral) â€” no API costs at scale, full control, but infrastructure overhead
- **Cost optimization** â€” batch evaluation calls, cache common translations, reduce MCQ regeneration

---

## âœ… Technical Architecture

### The system as built

```
data/
  concepts.yaml          â€” 61 concepts across 8 tiers (A1â†’A2)
  seed_words.json        â€” 531 words with emoji, examples, topic tags
  personas/*.yaml        â€” 4 persona soul files
  es_en_dictionary.json  â€” ~5000 Spanish-English translations

src/spanish_vibes/
  flow.py               â€” Card selection engine (teach â†’ word_intro â†’ word_match â†’ word_practice â†’ MCQ â†’ conversation)
  flow_routes.py        â€” HTTP endpoints, card serving, conversation management
  flow_ai.py            â€” MCQ generation via GPT-4o-mini
  flow_db.py            â€” Flow session DB queries
  conversation.py       â€” Chat engine, CEFR scaffolding, concept steering
  personas.py           â€” YAML loading, prompt building, engagement-weighted selection
  evaluation.py         â€” Post-conversation GPT-4o evaluation + enjoyment scoring
  memory.py             â€” Store/retrieve persona memories + user profile
  interest.py           â€” EMA interest scoring, topic matching, decay
  words.py              â€” Word lifecycle, seeding, harvesting, intro/practice/match cards
  lexicon.py            â€” Translation pipeline (cache â†’ dictionary â†’ AI)
  concepts.py           â€” Concept loading, topological sort, prerequisite checking
  bkt.py                â€” Bayesian Knowledge Tracing (P_L0=0.0, P_T=0.1, P_G=0.25, P_S=0.1)
  srs.py                â€” XP-based level (decorative)
  conversation_types.py â€” 6 conversation types with weighted selection
  db.py                 â€” SQLite schema, migrations, 30+ tables
```

### Data flow

```
User starts session
       â”‚
       â–¼
select_next_card() in flow.py
       â”‚
       â”œâ”€â”€ Concept selection: weighted buckets (spot 30%, practice 50%, new 20%)
       â”œâ”€â”€ Card type cascade: teach â†’ word_intro â†’ word_match â†’ word_practice â†’ MCQ
       â”œâ”€â”€ Conversation trigger: every 5th card if experienced
       â”‚
       â–¼
Card served â†’ User responds
       â”‚
       â”œâ”€â”€ MCQ answer â†’ BKT update + CardSignal (topic_id=None)
       â”œâ”€â”€ Word intro â†’ mark_word_introduced()
       â”œâ”€â”€ Word practice â†’ mark_word_practice_result()
       â”‚
       â””â”€â”€ Conversation ends
               â”‚
               â–¼
        Post-conversation evaluation (GPT-4o)
               â”‚
               â”œâ”€â”€ concepts_demonstrated â†’ BKT update (boosted weight)
               â”œâ”€â”€ vocabulary_used â†’ harvest_conversation_words()
               â”œâ”€â”€ user_facts â†’ store in user_profile
               â”œâ”€â”€ persona_observations â†’ store in persona_memories
               â”œâ”€â”€ engagement_quality â†’ enjoyment_score â†’ persona_engagement
               â””â”€â”€ interest signal â†’ InterestTracker.update_from_card_signal()
```

---

## ðŸ’¡ Open Questions

- Should the dimensional profile (vocab/grammar/fluency/comprehension) be visible to users? A radar chart could be motivating
- How aggressively should conversation evidence skip content? Too aggressive = gaps, too conservative = boredom
- How many conversations before persona engagement signal is reliable? Probably 3-5 per persona minimum
- For story comprehension: should the story be generated per-conversation or pre-cached?
- When does interest-driven vocabulary become counterproductive? (User only learns sports words, can't have a basic conversation about anything else)
- How to handle the transition from core vocabulary (everyone needs it) to interest-driven vocabulary (personalized)?
